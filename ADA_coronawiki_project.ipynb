{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGRDBIn4aA4t"
      },
      "source": [
        "# Analysis of the Coronawiki dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preliminary analysis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the different files and study each dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ORTl38zWm4e",
        "outputId": "26e70775-28e5-4261-db70-d426d1d679b2"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'geopandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-1-512621a827d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpatches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import geopandas as gpd\n",
        "import json\n",
        "import gzip\n",
        " \n",
        "\n",
        "\n",
        "path = 'data/'\n",
        "\n",
        "#Load csv files\n",
        "df_interventions = pd.read_csv(path + 'interventions.csv')\n",
        "df_google_mobility = pd.read_csv(path + 'Global_Mobility_Report.csv.gz')\n",
        "df_apple_mobility = pd.read_csv(path + 'applemobilitytrends-2020-04-20.csv')\n",
        "df_topics_linked = pd.read_csv(path + 'topics_linked.csv.xz')\n",
        "df_democracy = pd.read_csv(path + 'democracy.csv')\n",
        "\n",
        "#Load the (gzipped) json file\n",
        "with gzip.open(path + 'aggregated_timeseries.json.gz', \"rb\") as f:\n",
        "    json_timeseries = json.loads(f.read().decode(\"ascii\"))\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Democracy Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Setting the Years after 1960 as indexes in order to plot the evolution of the democracy index \n",
        "df_democracy_plot=df_democracy[df_democracy['Year']>=1960]\n",
        "df_democracy_plot.set_index('Year',inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the evolution of the democracy index for a selection of countries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Grouping the countries and ploting their evolution.\n",
        "df_democracy_plot[(df_democracy_plot['Entity'].isin(['Denmark','Finland','Germany','Italy','Japan','Netherlands','Norway','Serbia','South Korea', 'Spain', 'Sweden', 'Turkey']))].groupby('Entity')['libdem_vdem_owid'].plot(x='Year',y='libdem_vdem_owid',legend=True)\n",
        "\n",
        "#Drawing the limit of the year 2019 since this is the year where we will consider the democracy score for the countries for our study\n",
        "plt.axvline(x = 2020, color = 'r',ls='--', label = 'Year = 2020')\n",
        "plt.title('Evolution of the Democracy Index')\n",
        "plt.legend(bbox_to_anchor=(1.0, 1.0))\n",
        "plt.show()\n",
        "#resent the index from the Year to before the plotting...\n",
        "df_democracy_plot.reset_index(inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the 2020 democracy index on a map for the world, Europe and East Asia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import the geopandas database in order to plot the maps\n",
        "countries = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#change the name of the countries in order to correctly merge the two dataframe.\n",
        "df_democracy=df_democracy.replace(\"United States\", \"United States of America\")\n",
        "df_democracy=df_democracy.replace(\"Democratic Republic of Congo\", \"Dem. Rep. Congo\")\n",
        "df_democracy=df_democracy.replace(\"Central African Republic\",\"Central African Rep.\")\n",
        "df_democracy=df_democracy.replace(\"Cote d'Ivoire\",\"CÃ´te d'Ivoire\")\n",
        "\n",
        "\n",
        "#merge with the democracy index database in order to associate to each of our wanted country a democracy score.\n",
        "countries = countries.merge(df_democracy[df_democracy[\"Year\"]==2020][['Entity','libdem_vdem_owid']],left_on='name',right_on='Entity',how='left')\n",
        "countries.drop('Entity', inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Plotting and drawing the maps with the democracy scores.\n",
        "\n",
        "ax1=countries.plot(column='libdem_vdem_owid', legend=True,legend_kwds={'label': \"Democracy Index\",\n",
        "                        'orientation': \"vertical\"}, missing_kwds={'color': 'lightgrey'})\n",
        "ax2=countries.plot(column='libdem_vdem_owid', legend=True,legend_kwds={'label': \"Democracy Index\",\n",
        "                        'orientation': \"vertical\"}, missing_kwds={'color': 'lightgrey'})\n",
        "ax3=countries.plot(column='libdem_vdem_owid', legend=True,legend_kwds={'label': \"Democracy Index\",\n",
        "                        'orientation': \"horizontal\"}, missing_kwds={'color': 'lightgrey'},figsize=(20, 10))\n",
        "ax1.set_xlim(-20,40)\n",
        "ax1.set_ylim(30,75)\n",
        "ax1.set_axis_off()\n",
        "\n",
        "ax2.set_xlim(80,150)\n",
        "ax2.set_ylim(0,45)\n",
        "ax2.set_axis_off()\n",
        "\n",
        "ax3.set_ylim(-70,90)\n",
        "ax3.set_axis_off()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interventions dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Dataframe overview\n",
        "\n",
        "df_interventions.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Countries considered in this dataset\n",
        "\n",
        "print (df_interventions.shape[0], 'countries considered :')\n",
        "\n",
        "for ctry in df_interventions['lang'] :\n",
        "    print (ctry, end='  ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apple mobility report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Dataframe overview\n",
        "\n",
        "df_apple_mobility.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Min and max dates of this dataset\n",
        "\n",
        "apple_cols = df_apple_mobility.columns\n",
        "print ('Reference day :', apple_cols[3])\n",
        "print ('Last day :', apple_cols[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Regions considered\n",
        "\n",
        "#Number of cities and country/region considered\n",
        "print (df_apple_mobility['geo_type'].value_counts(), '\\n')\n",
        "\n",
        "#Regions considered\n",
        "apple_regions = df_apple_mobility[df_apple_mobility['geo_type']=='country/region']['region']\n",
        "print ('Regions : \\n', apple_regions.unique(), '\\n')\n",
        "\n",
        "#Count the regions where the data is for walking, driving and transit\n",
        "occurence_counting = apple_regions.value_counts()\n",
        "print('Nb of regions with data for walking, driving and transit :', occurence_counting[occurence_counting==3].shape[0])\n",
        "print('Nb of regions with data for only walking and driving :', occurence_counting[occurence_counting==2].shape[0], '\\n')\n",
        "\n",
        "#Cities considered\n",
        "apple_cities = df_apple_mobility[df_apple_mobility['geo_type']=='city']['region']\n",
        "print ('Cities : \\n', apple_cities.unique(), '\\n')\n",
        "\n",
        "#Count the regions where the data is for walking, driving and transit\n",
        "occurence_counting = apple_cities.value_counts()\n",
        "print('Nb of cities with data for walking, driving and transit :', occurence_counting[occurence_counting==3].shape[0])\n",
        "print('Nb of cities with data for only walking and driving :', occurence_counting[occurence_counting==2].shape[0], '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Exemple : plot walking evolution in France\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Extract walking data for France\n",
        "france_walking = df_apple_mobility[(df_apple_mobility['region']=='France') & (df_apple_mobility['transportation_type']=='walking')]\n",
        "france_walking.drop(['geo_type', 'region', 'transportation_type'], inplace=True, axis=1)\n",
        "\n",
        "#Transpose it to have a pandas series and plot it\n",
        "france_walking = france_walking.transpose()\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "plt.plot(france_walking)\n",
        "plt.xticks(fontsize=7, rotation=90)\n",
        "plt.title('Walking Evolution in France')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Relative evolution (%)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Google mobility report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Overview of the dataset\n",
        "\n",
        "df_google_mobility.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Countries for which we have data\n",
        "google_countries = df_google_mobility['country_region'].unique()\n",
        "print ('Data for', google_countries.size, 'countries :')\n",
        "print (google_countries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#The data can be at a more specific scale than the whole country\n",
        "#For instance in France, the different regions are considered\n",
        "\n",
        "google_mob_france = df_google_mobility[df_google_mobility['country_region'] == 'France']\n",
        "print ('Regions considered in France')\n",
        "print (google_mob_france['sub_region_1'].unique(), '\\n')\n",
        "\n",
        "#Let's now focus on Occitanie\n",
        "print ('Subregions in Occitanie :')\n",
        "google_mob_occ = google_mob_france[google_mob_france['sub_region_1'] == 'Occitanie']\n",
        "print (google_mob_occ['sub_region_2'].unique())\n",
        "\n",
        "#In occitanie let's focus on Haute-Garonne\n",
        "google_mob_hg = google_mob_occ[google_mob_france['sub_region_2'] == 'Haute-Garonne']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Now let's see what data can be extracted in Haute-Garonne\n",
        "print ('Columns of the dataframe :')\n",
        "print (google_mob_hg.columns, '\\n')\n",
        "\n",
        "#Focus on the date\n",
        "print ('Date range : ')\n",
        "print ('from', google_mob_hg['date'].iloc[0], 'to', google_mob_hg['date'].iloc[-1], '\\n')\n",
        "\n",
        "#Specific places for which we have data\n",
        "print ('Types of places considered :')\n",
        "for col in google_mob_hg.columns :\n",
        "    if '_percent_change_from_baseline' in col :\n",
        "        print(' -',col.replace('_percent_change_from_baseline', ''))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis of the dataframe topics_linked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print the shape and head of the dataframe\n",
        "\n",
        "print ('Shape of the dataframe :', df_topics_linked.shape)\n",
        "df_topics_linked.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Print the Wikipedia pages related to a specific topic\n",
        "\n",
        "#Example for east Asia\n",
        "topic = 'Geography.Regions.Asia.East Asia'\n",
        "df_specific_topic = df_topics_linked[df_topics_linked[topic]==True]\n",
        "print ('The', df_specific_topic.shape[0], 'Wikipedia pages related to', topic, 'are : \\n')\n",
        "print (df_specific_topic['index'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis of the dictionary json_timeseries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The coronawiki time series data are contained in the dictionary json_timeseries, let's see what is the structure of this dictionary\n",
        "versions = list(json_timeseries.keys())\n",
        "versions.sort()\n",
        "\n",
        "print('The different versions of Wikipedia that are considered are:')\n",
        "print(versions)\n",
        "\n",
        "print('We have the visit history on '+str(int(len(versions)/2))+' different versions of Wikipedia, for each of these versions there are two keys one for the normal version and one for the mobile version .m ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now let's see what is the structure inside one of these version, the german one for example\n",
        "german = json_timeseries['de']\n",
        "print('The keys for the german version, and for all other versions are:')\n",
        "print(list(german.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The key len gives the number of Wikipedia pages for this version\n",
        "print('key len: The german version of Wikipedia contains '+str(int(german['len']))+' pages \\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The key sum gives the number of daily visits to all pages between 01-01-2018 and 07-31-2020\n",
        "total_visits = german['sum']\n",
        "print('key sum: The daily number of visits for these pages during the ten first days of 2018 are:')\n",
        "print(list(total_visits.values())[:10])\n",
        "print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now let's see what is inside the key covid\n",
        "covid_visits = german['covid']\n",
        "print('The key covid contains 3 sub-keys: ')\n",
        "print(list(covid_visits.keys()))\n",
        "# First, the number of pages that are considered to be linked with COVID-19 is in the key len\n",
        "print('key len: ' + str(covid_visits['len'])+' pages are linked with COVID-19')\n",
        "# Then, the key sum gives the number of daily visits to pages linked with COVID-19 between 01-01-2018 and 07-31-2020\n",
        "print('key sum: The daily number of visits for COVID-19 related pages during the ten first days of 2018 are:')\n",
        "print(list(covid_visits['sum'].values())[:10])\n",
        "# Finally, the key percent gives the proportion of COVID-related visits to total visits\n",
        "print('key percent: The proportion of COVID-related visits to total visits during the ten first days of 2018 are:')\n",
        "print(list(covid_visits['percent'].values())[:10])\n",
        "print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finally the key topics contains a sub-key for each topic that are detailed in the dataframe df_topics_linked.\n",
        "print('key topics: '+str(len(german['topics'].keys())) + ' topics are considered in this version')\n",
        "# Inside one of these topics the structure is the same as in the key covid:\n",
        "print('Structure of a topic: ') \n",
        "print(list(german['topics']['Culture.Biography.Biography*'].keys()))\n",
        "print('\\n')\n",
        "# First, the number of pages that are considered to be linked with this topic is in the key len\n",
        "print('key len: ' + str(german['topics']['Culture.Biography.Biography*']['len'])+' pages are linked with biography \\n')\n",
        "# Then, the key sum gives the number of daily visits to pages linked with this topic between 01-01-2018 and 07-31-2020\n",
        "print('key sum: The daily number of visits for pages related to biography during the ten first days of 2018 are:')\n",
        "print(list(german['topics']['Culture.Biography.Biography*']['sum'].values())[:10])\n",
        "print('\\n')\n",
        "# Finally, the key percent gives the proportion of visits related to this topic to total visits\n",
        "print('key percent: The proportion of visits related to biography to total visits during the ten first days of 2018 are:')\n",
        "print(list(german['topics']['Culture.Biography.Biography*']['percent'].values())[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The structure of the dictionary json_timeseries can be shown with the following graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(\"json.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# First analysis of the evolution of interests"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis of the total number of wikipedia searches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a list of the names of the wikipedia versions.\n",
        "wiki_versions =['sv','de','it','sr','no','ko','da','ja','nl','fi','ca','tr']\n",
        "\n",
        "\n",
        "#Here we want to find the dat eof the beginning of the lockdown and insert it in the dictionnary in order to later plot it\n",
        "lockdown_start_dic={'sv':0,'de':0,'it':0,'sr':0,'no':0,'ko':0,'da':0,'ja':0,'nl':0,'fi':0,'ca':0,'tr':0}\n",
        "\n",
        "\n",
        "def find_lockdown_start(version):\n",
        "    # Let's put in the list time the list of the days of the considered period\n",
        "    time = list(json_timeseries[version]['sum'].keys())\n",
        "    # We just remove the hours\n",
        "    time = [t[:10] for t in time]\n",
        "\n",
        "    # Thanks to the dataframe df_interventions we can get the dates of the lockdown in the associated country\n",
        "    lockdown_start = df_interventions.loc[df_interventions['lang'] == version]['Mobility'].to_list()\n",
        "    # We compute the index of these dates in the list of the days\n",
        "    start_index = time.index(lockdown_start[0])-365\n",
        "    return lockdown_start[0],start_index\n",
        "\n",
        "#compute the start lockdown indexes and store them in the dictionnary    \n",
        "for version in wiki_versions:\n",
        "    lockdown_start_dic[version]=find_lockdown_start(version)[1]\n",
        "    \n",
        "print(lockdown_start_dic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#This is a function that gives us the x and y axis corresponding to the time stamps starting the year 2019 for the x-axis,\n",
        "# and the number of searches per days for the y-axis\n",
        "\n",
        "def wiki_searches_axis(version):\n",
        "    V=json_timeseries[version]['sum']\n",
        "    myList = V.items()\n",
        "    myList = sorted(myList) \n",
        "    x, y = zip(*myList) \n",
        "    return np.array(x[365:]),np.array(y[365:])\n",
        "\n",
        "# This function calculates the rolling average of an array x over a window of width w\n",
        "def moving_average(x, w):\n",
        "    return np.convolve(x, np.ones(w), 'valid') / w\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_wiki_searches(version):\n",
        "\n",
        "    # Let's put in the list time the list of the days of the considered period\n",
        "    time = list(json_timeseries[version]['sum'].keys())\n",
        "    # We just remove the hours\n",
        "    time = [t[:10] for t in time]\n",
        "\n",
        "    # For the purpose of plotting our results we create a list of days with only 1 day in 21 and we also create a list of index for these days\n",
        "    time_reduced = [time[21*i+365] for i in range(int(len(time[365:])/21)+1)]\n",
        "    time_pos = [21*i for i in range(int(len(time[365:])/21)+1)]\n",
        "\n",
        "\n",
        "    lockdown_start,start_index = find_lockdown_start(version)\n",
        "    # We compute the index of these dates in the list of the days\n",
        "    start_index = time.index(lockdown_start)-365\n",
        "    time_pos.append(start_index)\n",
        "    time_reduced.append(lockdown_start+' Start')\n",
        "\n",
        "    # We also remove two points which are too close from the start/end of the lockdown to have a nice plot\n",
        "    time_pos.pop(time_reduced.index('2020-03-17'))\n",
        "    time_reduced.pop(time_reduced.index('2020-03-17'))\n",
        "    \n",
        "    #finding our axis\n",
        "    x,y=wiki_searches_axis(version)\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(15,5))\n",
        "    fig.suptitle(\"Evolution of Wikipedia searches for {version}\".format(version=version))\n",
        "\n",
        "    #compute the rolling mean\n",
        "    rolling_mean=np.array(moving_average(y,7))\n",
        "    \n",
        "    #Plotting everything\n",
        "    ax1.plot(x, y, alpha=0.3)\n",
        "    ax1.plot(x[:len(x)-6],rolling_mean)\n",
        "    ax1.axvline(x = lockdown_start_dic[version], color = 'r',ls='--', label = 'Year = 2020')\n",
        "    \n",
        "    \n",
        "    ax1.set_xticks(time_pos,time_reduced,fontsize=10)\n",
        "    ax1.tick_params('x',labelrotation=90)\n",
        "    plt.show()\n",
        "\n",
        "for version in wiki_versions:\n",
        "    plot_wiki_searches(version)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#dictionnary with the population of each country\n",
        "country_population={'sv':10.28e6,'de':83.09e6,'it':59.73e6,'sr':6.945e6,'no':5.348e6,'ko':51.76e6,'da':5.814e6,'ja':126.6e6,'nl':17.34e6,'fi':5.522e6,'ca':7.566e6,'tr':83.43e6}\n",
        "\n",
        "#plot the mean on the wikipedia searches for the countries, weighted according to the country's population.\n",
        "def plot_sum_wiki():\n",
        "    sum_searches=np.zeros(578)\n",
        "    \n",
        "    # Let's put in the list time the list of the days of the considered period\n",
        "    time = list(json_timeseries['it']['sum'].keys())\n",
        "    # We just remove the hours\n",
        "    time = [t[:10] for t in time]\n",
        "\n",
        "    # For the purpose of plotting our results we create a list of days with only 1 day in 21 and we also create a list of index for these days\n",
        "    time_reduced = [time[21*i+365] for i in range(int(len(time[365:])/21)+1)]\n",
        "    time_pos = [21*i for i in range(int(len(time[365:])/21)+1)]\n",
        "    \n",
        "    #compute the mean value of the searches\n",
        "    for K in wiki_versions:\n",
        "        x,y=wiki_searches_axis(K)\n",
        "        sum_searches=np.add(sum_searches,country_population[K]*y)\n",
        "    sum_searches=sum_searches/sum(country_population.values())\n",
        "\n",
        "    #compute the rolling mean\n",
        "    rolling_mean=np.array(moving_average(sum_searches,7))\n",
        "    \n",
        "\n",
        "    # plotting\n",
        "    fig, ax1 = plt.subplots(figsize=(15,5))\n",
        "    fig.suptitle(\"Evolution of Wikipedia searches for all countries\")\n",
        "    ax1.plot(x, sum_searches, alpha=0.3)\n",
        "    ax1.plot(x[:len(x)-6],rolling_mean)\n",
        "    \n",
        "    #setting the ticks\n",
        "    ax1.set_xticks(time_pos,time_reduced,fontsize=10)\n",
        "    ax1.tick_params('x',labelrotation=90)\n",
        "    plt.show()\n",
        "\n",
        "plot_sum_wiki()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Here we want to plot how each topic evolved during the pandemics with respect to the normal\n",
        "\n",
        "# Let's put in the list time the list of the days of the considered period\n",
        "time = list(json_timeseries['it']['topics']['Culture.Sports']['sum'].keys())\n",
        "# We just remove the hours\n",
        "time = [t[:10] for t in time]\n",
        "\n",
        "# Thanks to the dataframe df_interventions we can get the dates of the lockdown in Italy\n",
        "lockdown_start = df_interventions.loc[df_interventions['lang'] == 'it']['Lockdown'].to_list()\n",
        "lockdown_end = df_interventions.loc[df_interventions['lang'] == 'it']['Normalcy'].to_list()\n",
        "\n",
        "# We compute the index of these dates in the list of the days\n",
        "start_index = time.index(lockdown_start[0])\n",
        "end_index = time.index(lockdown_end[0])\n",
        "\n",
        "#Create an empty dataframe which will contain the relative evolution for each topic\n",
        "df = pd.DataFrame({'topic' : [], 'evolution' : []})\n",
        "\n",
        "#For each topic, compute its evolution and add it to the dataframe\n",
        "for topic in json_timeseries['it']['topics'].keys() :\n",
        "    topic_m = pd.Series(list(json_timeseries['it.m']['topics'][topic]['percent'].values()))\n",
        "    topic_normal = pd.Series(list(json_timeseries['it']['topics'][topic]['percent'].values()))\n",
        "    topic_all = topic_normal.add(topic_m)\n",
        "\n",
        "    #Relative evolution : ratio between the relative topic interset during the pandemic vs before pandemic\n",
        "    relative_evolution = topic_all[start_index:end_index].mean() / topic_all.mean()\n",
        "\n",
        "    #Select only the topics where the evolution is strong\n",
        "    threshold=0.0 #can be increased if you want only the topics which have change above the threshold\n",
        "    if relative_evolution > 1+threshold or relative_evolution < 1-threshold:\n",
        "        new_entry = [topic, (relative_evolution-1)*100]\n",
        "        df.loc[df.shape[0]] = new_entry \n",
        "\n",
        "#Sort and plot the dataframe\n",
        "df.sort_values(by=['evolution'], inplace=True)\n",
        "plt.figure(figsize=(10, 15))\n",
        "plt.xlabel('Relative evolution (%)')\n",
        "plt.barh(df['topic'], df['evolution'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On this plot, the observed trend seems to be that italians' interest during lockdown decreased overall for things that were forbidden (travelling and doing sports), and increased overall for things that were allowed (watching films and reading books). The Italians thus seem to have conformed to lockdown rather than to be interested in what was no longer accessible.\n",
        "Nevertheless, there are some exceptions like the categories Southern and North Africa that have increased, or the category Music that has decreased.\n",
        "\n",
        "To develop this work we should do the same kind of analysis for different countries, especially in countries where the lockdown conditions were different.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's plot the evolution of the relative number of visits for two topics that have a large variation: Sports and Books\n",
        "\n",
        "# For books and sports we create a list with the daily relative number of visits on the italian version linked to these topics\n",
        "books = pd.Series(list(json_timeseries['it']['topics']['Culture.Media.Books']['percent'].values()))\n",
        "sport = pd.Series(list(json_timeseries['it']['topics']['Culture.Sports']['percent'].values()))\n",
        "\n",
        "# We do the same on the mobile version of the italian Wikipedia\n",
        "books_m = pd.Series(list(json_timeseries['it.m']['topics']['Culture.Media.Books']['percent'].values()))\n",
        "sport_m = pd.Series(list(json_timeseries['it']['topics']['Culture.Sports']['percent'].values()))\n",
        "\n",
        "# For these two topics we add the values for the mobile and classic version\n",
        "books_tot = books.add(books_m)\n",
        "sport_tot = sport.add(sport_m)\n",
        "\n",
        "# For the purpose of plotting our results we create a list of days with only 1 day in 21 and we also create a list of index for these days\n",
        "time_reduced = [time[21*i] for i in range(int(len(time)/21)+1)]\n",
        "time_pos = [21*i for i in range(int(len(time)/21)+1)]\n",
        "\n",
        "# We add to the reduced list of days and the list of their index the dates of lockdowns in Italy\n",
        "time_pos.append(start_index)\n",
        "time_reduced.append(lockdown_start[0]+' Start')\n",
        "\n",
        "time_pos.append(end_index)\n",
        "time_reduced.append(lockdown_end[0]+' End')\n",
        "\n",
        "# We also remove two points which are too close from the start/end of the lockdown to have a nice plot\n",
        "time_pos.pop(time_reduced.index('2020-03-09'))\n",
        "time_reduced.pop(time_reduced.index('2020-03-09'))\n",
        "\n",
        "time_pos.pop(time_reduced.index('2020-06-22'))\n",
        "time_reduced.pop(time_reduced.index('2020-06-22'))\n",
        "\n",
        "fig, [ax1,ax2] = plt.subplots(2,1,figsize=(20,20),sharex=True)\n",
        "ax1.plot(books_tot.rolling(7).mean(),label='Books')\n",
        "ax2.plot(sport_tot.rolling(7).mean(),color='green',label='Sports')\n",
        "ax2.set_xticks(time_pos,time_reduced,fontsize=10)\n",
        "ax2.tick_params('x',labelrotation=90)\n",
        "ax1.axvline(x=start_index)\n",
        "ax1.axvline(x=end_index)\n",
        "ax2.axvline(x=start_index)\n",
        "ax2.axvline(x=end_index)\n",
        "fig.suptitle('Evolution of the rolling mean over 7 days of the relative number \\n of visits linked with Books and Sports on the italian Wikipedia',fontsize=20)\n",
        "\n",
        "ax1.legend(fontsize=15)\n",
        "ax2.legend(fontsize=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These two graphs confirm the previous results and give a better view of the overall evolution of interest in sport and books in Italy. Interest in books peaks (over the period considered) about a week after the start of the lockdown and remains higher overall during the lockdown than over the rest of the period. It is also clearly visible that this interest starts to decrease after several weeks of lockdown and eventually drops when the situation returns to normal. \n",
        "\n",
        "On the contrary, interest in sport reaches its lowest level about a week after lockdown, increases slowly over the weeks and then very quickly when returning to normal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Like the previous bar plot, the objective is here to be able to visualize the interest evolution of all the subjects but for each country and on the same figure, in a simple and clear way\n",
        "# For this we will build a matrix whose color gradient will correspond to the interest evolution of a certain subject in a certain country\n",
        "import datetime\n",
        "import time\n",
        "from datetime import date\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "\n",
        "# First we create a dataframe df_p4 with the data we need for this final part, the column versions contains the two letters corresponding the versions of Wikipedia that are considered, the column dem_index contains the democracy index of the corresponding countries and the column mobility shift contains the relative mobility shift computed previously.\n",
        "versions = ['tr', 'sr', 'ja', 'it', 'ko', 'ca', 'nl', 'de', 'fi', 'no', 'sv', 'da']\n",
        "dem_index = [0.11, 0.27, 0.74, 0.78, 0.79, 0.8, 0.81, 0.83, 0.83, 0.86, 0.88, 0.88]\n",
        "mobility_shift = [-0.777, -0.785, -0.514, -0.904, -0.640 , -0.943, -0.657, -0.620, -0.416, -0.544, -0.418, -0.525]\n",
        "dict = {'country': versions, 'democracy index': dem_index, 'mobility shift': mobility_shift}\n",
        "df_p4 = pd.DataFrame(data = dict)\n",
        "\n",
        "# To facilitate the building of the final matrix we sort the dataframe either with the democracy index or with the mobility shift\n",
        "sort = 'mobility shift' # can be changed with 'dem_index' if we want the output as the function of the democracy index\n",
        "df_p4 = df_p4.sort_values(sort) \n",
        "df_p4 = df_p4.reset_index(drop = True)\n",
        "\n",
        "# Now we build a list period with all the days (with the datetime type) of the considered period\n",
        "period = []\n",
        "d0 = date(2018,1,1) # Day of start\n",
        "d1 = date(2020,7,31) # Day of end\n",
        "delta = datetime.timedelta(days=1) # Duration between two points: one day\n",
        "\n",
        "while d0 <= d1:\n",
        "    period.append(d0.strftime('%Y-%m-%d'))\n",
        "    d0 += delta \n",
        "\n",
        "# Thanks to the dataframe df_interventions we can get the dates of the lockdown in every countries that we put in a new dataframe lockdown\n",
        "lockdown = df_interventions.loc[df_interventions['lang'].isin(df_p4['country'])][['lang','Mobility','Normalcy']]\n",
        "\n",
        "# There are two missing values: the return to normalcy in Catalonia and Turkey. We will assume that the return to normalcy is 2020-07-01 for these countries\n",
        "lockdown.iloc[8,2] = '2020-07-01'\n",
        "lockdown.iloc[11,2] = '2020-07-01'\n",
        "\n",
        "# We compute the index of these dates in the list of the days and we put them in two new columns 'start_index' and 'end_index'\n",
        "lockdown['start_index'] = [period.index(date) for date in lockdown['Mobility']]\n",
        "lockdown['end_index'] = [period.index(date) for date in lockdown['Normalcy']]\n",
        "\n",
        "# We create a dataframe where each row is characterized by a country in the column 'country', a topic in the column 'topic' and the relative evolution of interest for this topic in this country\n",
        "df_evolution = pd.DataFrame({'country' : [], 'topic' : [], 'evolution' : []})\n",
        " \n",
        "#For each topic and each country, we compute its evolution and add it to the dataframe\n",
        "for d in df_p4['country']:\n",
        "    for topic in json_timeseries[d]['topics'].keys() :\n",
        "        # We get the daily data of the share of this topic in the global wikipedia searches in this country (mobile and desktop version)\n",
        "        topic_percent_mobile = pd.Series(list(json_timeseries[d + '.m']['topics'][topic]['percent'].values()))\n",
        "        topic_percent_desktop = pd.Series(list(json_timeseries[d]['topics'][topic]['percent'].values()))\n",
        "\n",
        "        # We get the daily data of wikipedia visits related to this topic in this country (mobile and desktop version)\n",
        "        topic_total_mobile = pd.Series(list(json_timeseries[d + '.m']['topics'][topic]['sum'].values()))\n",
        "        topic_total_desktop = pd.Series(list(json_timeseries[d]['topics'][topic]['sum'].values()))\n",
        "\n",
        "        # To know the proportion that this subject represents in the wikipedia visits we combine the data of the mobile version and the desktop version by making a weighted average of the two versions\n",
        "        topic_all = (topic_percent_mobile * topic_total_mobile + topic_percent_desktop * topic_total_desktop) / (topic_total_mobile + topic_total_desktop)\n",
        "\n",
        "        # We compute the relative evolution which is defined as the ratio between the average topic interest during the pandemic vs the average topic interest during the saeme period but one year before\n",
        "\n",
        "        # Index of the day of the start and end of the lockdown in the list of the days\n",
        "        start_index = lockdown.loc[lockdown['lang'] == d]['start_index'].tolist()[0]\n",
        "        end_index = lockdown.loc[lockdown['lang'] == d]['end_index'].tolist()[0]\n",
        "\n",
        "        relative_evolution = (topic_all[start_index:end_index].mean() -  topic_all[start_index - 365 : end_index - 365].mean()) / topic_all[start_index - 365 : end_index - 365].mean()\n",
        "\n",
        "        # We create a new entry for the output dataframe with all the data needed\n",
        "        new_entry = [d, topic, relative_evolution*100]\n",
        "        df_evolution.loc[df_evolution.shape[0]] = new_entry \n",
        "\n",
        "# Now let's build the final matrix\n",
        "\n",
        "# The number of rows in the number of different topics in the dataframe df_evolution\n",
        "topics = list(set(df_evolution['topic'].tolist()))\n",
        "nb_rows = len(topics)\n",
        "\n",
        "# The number of columns is the number of countries\n",
        "countries = df_p4['country']\n",
        "nb_columns = len(countries)\n",
        "\n",
        "# We create a matrix full of zeros with an additional column that will contain the average of all others columns\n",
        "A = np.zeros((nb_rows, nb_columns + 1))\n",
        "\n",
        "# Let's fill this matrix\n",
        "for i in range(nb_rows):\n",
        "    for j in range(nb_columns):\n",
        "        # For each element of the matrix we retrieve the corresponding data in the DataFrame built previously\n",
        "        A[i,j] = df_evolution.loc[(df_evolution['country'] == countries[j]) & (df_evolution['topic'] == topics[i])]['evolution'].tolist()[0]\n",
        "    # The last column is the average evolution of the corresponding subject over all countries\n",
        "    A[i,nb_columns] = sum([A[i,j] for j in range(nb_columns)]) / nb_columns\n",
        "\n",
        "# Let's remove some topics whose evolution is not significant: if the average evolution is less than 10 we remove the corresponding row\n",
        "# The threshold can be modified\n",
        "removed_rows = []\n",
        "for i in range(nb_rows):\n",
        "    if abs(A[i, nb_columns]) <= 10:\n",
        "        removed_rows.append(i)\n",
        "\n",
        "A = np.delete(A, removed_rows, axis = 0)\n",
        "\n",
        "# We also remove the corresponding topics in the list of the topics\n",
        "topics = np.delete(topics, removed_rows)\n",
        "\n",
        "# Then we sort the matrix (and the list of the topics) according to the value of the last column\n",
        "l = A[:, nb_columns].argsort()\n",
        "\n",
        "A = A[l]\n",
        "topics = np.array(topics)[l]\n",
        "\n",
        "# We can delete the last column before plotting the result\n",
        "A = np.delete(A, (nb_columns), axis = 1)\n",
        "\n",
        "# Let's plot our result\n",
        "fig = plt.figure(figsize=(30,15))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "'''\n",
        "Function to offset the \"center\" of a colormap. Useful for data with a negative min and positive max and you want the middle of the colormap's dynamic range to be at zero.\n",
        "\n",
        "Input\n",
        "-----\n",
        "    cmap : The matplotlib colormap to be altered\n",
        "    start : Offset from lowest point in the colormap's range.\n",
        "        Defaults to 0.0 (no lower offset). Should be between 0.0 and `midpoint`.\n",
        "    midpoint : The new center of the colormap. Defaults to 0.5 (no shift). Should be between 0.0 and 1.0. In general, this should be  1 - vmax / (vmax + abs(vmin))\n",
        "        For example if your data range from -15.0 to +5.0 and you want the center of the colormap at 0.0, `midpoint` should be set to  1 - 5/(5 + 15)) or 0.75\n",
        "    stop : Offset from highest point in the colormap's range.\n",
        "        Defaults to 1.0 (no upper offset). Should be between `midpoint` and 1.0.\n",
        "'''\n",
        "def shiftedColorMap(cmap, start=0, midpoint=0.5, stop=1.0, name='shiftedcmap'):\n",
        "    cdict = {\n",
        "        'red': [],\n",
        "        'green': [],\n",
        "        'blue': [],\n",
        "        'alpha': []\n",
        "    }\n",
        "\n",
        "    # regular index to compute the colors\n",
        "    reg_index = np.linspace(start, stop, 257)\n",
        "\n",
        "    # shifted index to match the data\n",
        "    shift_index = np.hstack([\n",
        "        np.linspace(0.0, midpoint, 128, endpoint=False), \n",
        "        np.linspace(midpoint, 1.0, 129, endpoint=True)\n",
        "    ])\n",
        "\n",
        "    for ri, si in zip(reg_index, shift_index):\n",
        "        r, g, b, a = cmap(ri)\n",
        "\n",
        "        cdict['red'].append((si, r, r))\n",
        "        cdict['green'].append((si, g, g))\n",
        "        cdict['blue'].append((si, b, b))\n",
        "        cdict['alpha'].append((si, a, a))\n",
        "\n",
        "    newcmap = matplotlib.colors.LinearSegmentedColormap(name, cdict)\n",
        "    #plt.register_cmap(cmap=newcmap)\n",
        "\n",
        "    return newcmap\n",
        "\n",
        "mp = 1 - A[:, :nb_columns].max() / (A[:, :nb_columns].max() + abs(A[:, :nb_columns].min()))\n",
        "\n",
        "new_cmap = shiftedColorMap(matplotlib.cm.seismic, midpoint=mp)\n",
        "\n",
        "cax = ax.matshow(A, interpolation='nearest', cmap = new_cmap)\n",
        "fig.colorbar(cax)\n",
        "\n",
        "xaxis = np.arange(len(countries))\n",
        "yaxis = np.arange(len(topics))\n",
        "ax.set_xticks(xaxis)\n",
        "ax.set_yticks(yaxis)\n",
        "ax.set_xticklabels(countries)\n",
        "ax.set_yticklabels(topics)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "# Now we will focus on the last two lines of the matrix which correspond to video games and movies\n",
        "\n",
        "# We will do a plot where each point represents a version of wikipedia, with the mobility shift in the associated country on the x-axis and the gain in interest in films and video games on the y-axis (weighted average of the two subjects).\n",
        "\n",
        "# The x-axis will be the mobility shift\n",
        "X = df_p4['mobility shift']*100\n",
        "\n",
        "# In these two list we will put the total number of visits linked with films or video games in each country. This need to compute the weighted average in the next step.\n",
        "\n",
        "films_total = []\n",
        "videogames_total = []\n",
        "\n",
        "for d in df_p4['country']:\n",
        "    films_total.append(pd.Series(list(json_timeseries[d]['topics']['Culture.Media.Films']['sum'].values())).mean() + pd.Series(list(json_timeseries[d + '.m']['topics']['Culture.Media.Films']['sum'].values())).mean())\n",
        "    videogames_total.append(pd.Series(list(json_timeseries[d]['topics']['Culture.Media.Video games']['sum'].values())).mean() + pd.Series(list(json_timeseries[d + '.m']['topics']['Culture.Media.Video games']['sum'].values())).mean())\n",
        "\n",
        "films_total = np.array(films_total)\n",
        "videogames_total = np.array(videogames_total)\n",
        "\n",
        "# The y-axis is the average of the gain in interest in videogames and films weighted by the number of visits for these two topics\n",
        "Y = (np.array(A[16, :]) * films_total + np.array(A[17,:]) * videogames_total) / (films_total + videogames_total)\n",
        "\n",
        "colors = sns.color_palette('Set2')\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.scatter(X, Y, color = colors[0], label = 'Films and Video Games')\n",
        "plt.xlabel('Mobility shift (%)')\n",
        "plt.ylabel('Evolution of the interest (%)')\n",
        "\n",
        "for i, txt in enumerate(df_p4['country']):\n",
        "    plt.annotate(txt, (X[i], Y[i]))\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now we can do a linear regression to approximate the effect of the mobility shift in the interest in films and video games\n",
        "\n",
        "import statsmodels.formula.api as smf\n",
        "d = {'mobility_shift': X, 'interest': Y}\n",
        "df_reglin = pd.DataFrame(data = d)\n",
        "\n",
        "np.random.seed(2)\n",
        "\n",
        "mod = smf.ols(formula='interest ~ mobility_shift', data=df_reglin)\n",
        "\n",
        "res = mod.fit()\n",
        "\n",
        "print(res.summary())\n",
        "\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.scatter(X, Y, color = colors[0], label = 'Films and Video Games')\n",
        "plt.plot(np.linspace(-100,-40,3), [-8.1772 - 0.5804 * k for k in np.linspace(-100,-40,3)], color = colors[1])\n",
        "plt.text(-80, -8.1772 - 0.5804 * (-80), 'y = -8.1772 - 0.5804 * x', color = colors[1])\n",
        "plt.xlabel('Mobility shift (%)')\n",
        "plt.ylabel('Evolution of the interest (%)')\n",
        "\n",
        "for i, txt in enumerate(df_p4['country']):\n",
        "    plt.annotate(txt, (X[i], Y[i]))\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of the change in mobility during the covid pandemic"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting the mobility evolution in a selection of countries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Exemple : plot walking evolution in France\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Getting sns colors\n",
        "colors = sns.color_palette(\"rocket\")\n",
        "\n",
        "#Extract walking data for France\n",
        "france_walking = df_apple_mobility[(df_apple_mobility['region']=='France') & (df_apple_mobility['transportation_type']=='walking')]\n",
        "france_walking.drop(['geo_type', 'region', 'transportation_type'], inplace=True, axis=1)\n",
        "\n",
        "#Extract data for Mexico\n",
        "mexico_walking = df_apple_mobility[(df_apple_mobility['region']=='Mexico') & (df_apple_mobility['transportation_type']=='walking')]\n",
        "mexico_walking.drop(['geo_type', 'region', 'transportation_type'], inplace=True, axis=1)\n",
        "\n",
        "\n",
        "#Extract data for Vietnam\n",
        "vietnam_walking = df_apple_mobility[(df_apple_mobility['region']=='Vietnam') & (df_apple_mobility['transportation_type']=='walking')]\n",
        "vietnam_walking.drop(['geo_type', 'region', 'transportation_type'], inplace=True, axis=1)\n",
        "\n",
        "#Transpose it to have a pandas series and plot it\n",
        "france_walking = france_walking.transpose()\n",
        "mexico_walking = mexico_walking.transpose()\n",
        "vietnam_walking = vietnam_walking.transpose()\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "plt.plot(france_walking.rolling(7, center=True).mean(), color=colors[2], label='France (democracy index = 0.798)')\n",
        "plt.plot(mexico_walking.rolling(7, center=True).mean(), color = colors[3], label='Mexico (democracy index = 0.412)')\n",
        "plt.plot(vietnam_walking.rolling(7, center=True).mean(), color = colors[4], label='Vietnam (democracy index = 0.112)')\n",
        "plt.plot(france_walking, alpha=.2, color=colors[0])\n",
        "plt.plot(mexico_walking, alpha=.2, color=colors[1])\n",
        "plt.plot(vietnam_walking, alpha=.2, color=colors[2])\n",
        "plt.xticks(fontsize=7, rotation=90)\n",
        "plt.title('Walking Evolution in France, Mexico and Vietnam')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Relative evolution (%)')\n",
        "plt.legend()\n",
        "#plt.savefig('images/walking_evolution_fr_me_viet.png')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of the link between mobility change and democracy index"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load democracy indexes from Our World in Data (https://ourworldindata.org/democracy) :\n",
        "\n",
        "\"Based on the expert assessments and index by V-Dem. It combines information on voting rights, the freedom and fairness of elections,freedoms of association and expression, civil liberties, and executive constraints. It ranges from 0 to 1 (most democratic)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the democracy index data\n",
        "democracy_index = pd.read_csv('data/liberal_democracy_index.csv')\n",
        "\n",
        "#Extracting the 2020 data\n",
        "democracy_index_2020 = democracy_index[democracy_index['Year']==2020]\n",
        "\n",
        "#Drop the useless columns and rename some columns\n",
        "democracy_index_2020.drop(['Year', 'libdem_vdem_low_owid', 'libdem_vdem_high_owid'], axis=1, inplace=True)\n",
        "democracy_index_2020.rename(columns={\"libdem_vdem_owid\": \"dem_index\"}, inplace=True)\n",
        "\n",
        "#Drop the data with nan values\n",
        "democracy_index_2020.dropna(inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Enrich the Google and Apple mobility reports with this data by merging the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Join with google mobility report\n",
        "df_google_mob_dem_index = df_google_mobility.merge(democracy_index_2020, left_on='country_region', right_on='Entity')\n",
        "\n",
        "#Join with Apple mobility report\n",
        "#Before, change the name of the country Republic of Korea as South Korea so that the merge will work for this country\n",
        "#df_apple_mobility.loc[df_apple_mobility.region=='Republic of Korea']['region'] = 'South Korea'\n",
        "df_apple_mobility.replace('Republic of Korea', 'South Korea', inplace=True)\n",
        "df_apple_mob_dem_index = df_apple_mobility.merge(democracy_index_2020, left_on='region', right_on='Entity')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the mobility drop in the Apple dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Add columns with the 2 highest and the two lowest weeks in terms of driving/walking\n",
        "df_apple_mob_dem_index['highest_period_value'] = np.nan\n",
        "df_apple_mob_dem_index['lowest_period_value'] = np.nan\n",
        "\n",
        "#Number of days of the period on which the mean is computed\n",
        "nb_days_period = 14\n",
        "\n",
        "#Iterate over all countries\n",
        "for i in range (df_apple_mob_dem_index.shape[0]) :\n",
        "    # Extract data about the current country\n",
        "    current_row = df_apple_mob_dem_index.iloc[i]\n",
        "    country = current_row['region']\n",
        "    #Keep only the numerical information\n",
        "    current_row.drop(['geo_type', 'region', 'transportation_type', 'Entity', 'Code', 'dem_index'], inplace=True)\n",
        "    #Compute the min and max means over 2 floating weeks\n",
        "    highest_period_value = current_row.rolling(nb_days_period).mean().max()\n",
        "    lowest_period_value = current_row.rolling(nb_days_period).mean().min()\n",
        "    #Add these values to the dataset\n",
        "    df_apple_mob_dem_index['highest_period_value'].iloc[i] = highest_period_value\n",
        "    df_apple_mob_dem_index['lowest_period_value'].iloc[i] = lowest_period_value\n",
        "\n",
        "#Considering that the highest mobility value corresponds to the pre lockdown period and the lowest mobility\n",
        "#corresponds to the lockdown period, we can compute the decrease in the mobility in percents\n",
        "df_apple_mob_dem_index['mob_variation'] = (df_apple_mob_dem_index['lowest_period_value'] - df_apple_mob_dem_index['highest_period_value']) / df_apple_mob_dem_index['highest_period_value']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the mobility change as a function of the democracy index. Display also the GDP per capita of the different countries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Loading the data about GDP per capita\n",
        "df_gdp = pd.read_csv('data/gdp-per-capita-worldbank.csv')\n",
        "\n",
        "#Keep only the 2020 values\n",
        "df_gdp = df_gdp[df_gdp.Year == 2020]\n",
        "\n",
        "#Rename the GDP column\n",
        "df_gdp.rename (columns = {'GDP per capita, PPP (constant 2017 international $)' : 'gdp_per_capita'}, inplace=True)\n",
        "\n",
        "#Drop country code and year columns\n",
        "df_gdp.drop(['Code', 'Year'], axis=1, inplace=True)\n",
        "\n",
        "#Add the log of the GDP to the dataframe\n",
        "df_gdp['log_gdp_per_capita'] = df_gdp['gdp_per_capita'].apply(np.log10)\n",
        "\n",
        "df_gdp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting the mobility change vs democracy index with visualization of the GDP per capita\n",
        "\n",
        "#Keep only the mobility variations for walking\n",
        "df_plot = df_apple_mob_dem_index[df_apple_mob_dem_index['transportation_type'] == 'walking']\n",
        "df_plot = df_plot[['region', 'dem_index', 'mob_variation']].merge(df_gdp, left_on='region', right_on='Entity')\n",
        "\n",
        "\n",
        "#Plotting the name of the countries\n",
        "plt.figure(figsize=(16, 7))\n",
        "for i in range (df_plot.shape[0]) :\n",
        "    plt.text(x=df_plot.iloc[i]['dem_index']+.006, y=df_plot.iloc[i]['mob_variation']-.01, s=df_plot.iloc[i]['region'], fontdict=dict(color='black',size=7), alpha=.5)\n",
        "\n",
        "#Scatter plot with the mob_change as a function of the democracy index\n",
        "plt.scatter(df_plot['dem_index'], df_plot['mob_variation'], c=df_plot['log_gdp_per_capita'], cmap='RdPu')\n",
        "\n",
        "#Adding titles and showing\n",
        "plt.title('Decrease in walking mobility with respect to the democracy index')\n",
        "plt.xlabel('Democracy index')\n",
        "plt.ylabel('Relative change in walking mobility')\n",
        "plt.colorbar(label='log of GDP per capita')\n",
        "#plt.savefig('images/decrease_walking_vs_dem_index.png')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Same thing but with a linear regression on the countries with a democracy index < 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Do the same but separating countries which have a dem index lower than .7 and countries > .7\n",
        "\n",
        "#Keep only the mobility variations for walking\n",
        "df_plot = df_apple_mob_dem_index[df_apple_mob_dem_index['transportation_type'] == 'walking']\n",
        "df_plot = df_plot[['region', 'dem_index', 'mob_variation']].merge(df_gdp, left_on='region', right_on='Entity')\n",
        "df_plot_democratic_countries = df_apple_mob_dem_index[(df_apple_mob_dem_index['transportation_type'] == 'walking') & (df_apple_mob_dem_index['dem_index'] > .7)]\n",
        "df_plot_democratic_countries = df_plot_democratic_countries[['region', 'dem_index', 'mob_variation']].merge(df_gdp, left_on='region', right_on='Entity')\n",
        "df_plot_undemocratic_countries = df_apple_mob_dem_index[(df_apple_mob_dem_index['transportation_type'] == 'walking') & (df_apple_mob_dem_index['dem_index'] <= .7)]\n",
        "df_plot_undemocratic_countries = df_plot_undemocratic_countries[['region', 'dem_index', 'mob_variation']].merge(df_gdp, left_on='region', right_on='Entity')\n",
        "\n",
        "\n",
        "#Plotting the name of the countries\n",
        "plt.figure(figsize=(16, 7))\n",
        "for i in range (df_plot_undemocratic_countries.shape[0]) :\n",
        "    plt.text(x=df_plot_undemocratic_countries.iloc[i]['dem_index']+.006, y=df_plot_undemocratic_countries.iloc[i]['mob_variation']-.01, s=df_plot_undemocratic_countries.iloc[i]['region'], fontdict=dict(color='black',size=7), alpha=.5)\n",
        "\n",
        "#Scatter plot with the mob_change as a function of the democracy index\n",
        "plt.scatter(df_plot['dem_index'], df_plot['mob_variation'], c=df_plot['log_gdp_per_capita'], cmap='RdPu', label='_nolegend_')\n",
        "plt.colorbar(label='log of GDP per capita')\n",
        "plt.scatter(df_plot_democratic_countries['dem_index'], df_plot_democratic_countries['mob_variation'], color=[.8, .8, .8], s=70, label='dem_index > 0.7')\n",
        "\n",
        "#Run linear regression and get coefficients\n",
        "mod = smf.ols(formula='mob_variation ~ dem_index', data=df_plot_undemocratic_countries)\n",
        "res = mod.fit()\n",
        "print (res.summary())\n",
        "coefficients = res.params.values\n",
        "\n",
        "#Plot the corresponding line\n",
        "x_line = np.array([df_plot['dem_index'].min(), 0.7])\n",
        "y_line = coefficients[0] + coefficients[1] * x_line\n",
        "plt.plot(x_line, y_line, 'k--', label='linear regression')\n",
        "\n",
        "\n",
        "#Adding titles and showing\n",
        "plt.title('Decrease in walking mobility with respect to the democracy index for countries with low democracy index (<0.7)')\n",
        "plt.xlabel('Democracy index')\n",
        "plt.ylabel('Relative change in walking mobility')\n",
        "plt.legend(loc='upper right')\n",
        "#plt.savefig('images/decrease_walking_vs_dem_index_with_regression.png')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Study the link between GDP per capita and democracy index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Now plot the results of the linear regression on the scatter plot\n",
        "\n",
        "#Getting sns colors\n",
        "colors = sns.color_palette('deep')\n",
        "\n",
        "#Scatter plot\n",
        "plt.figure (figsize=(10, 5))\n",
        "plt.scatter (df_dem_index_gdp['log_gdp_per_capita'],df_dem_index_gdp['dem_index'], color=colors[7], marker='+', label='Pair GWP/democracy index for different countries')\n",
        "\n",
        "#Linear regression\n",
        "x = np.linspace (df_dem_index_gdp['log_gdp_per_capita'].min(), df_dem_index_gdp['log_gdp_per_capita'].max(), 10)\n",
        "y = coefficients[1]*x + coefficients[0]\n",
        "plt.plot(x, y, color=colors[5], linestyle='--', label='linear regression')\n",
        "\n",
        "#Add titles and legend\n",
        "plt.title(\"Democracy index as a function of GWP\")\n",
        "plt.xlabel(\"Log of GDP per capita (2020)\")\n",
        "plt.ylabel('Democracy index')\n",
        "plt.legend()\n",
        "#plt.savefig('images/gdp_dem_index.png')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print (res.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clustering countries on the typology of their restrictions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each country, create an associated vector containing the mobility drop for each category. The goal is to cluster countries based on these values to evaluate the restrictions typology and compare it with the democracy index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Number of days on which we apply the rolling mean\n",
        "nb_days_period = 14\n",
        "\n",
        "#Create an empty dictionnary to store the results\n",
        "dict_restrictions = {}\n",
        "\n",
        "#Mobility categories which will be taken into account\n",
        "categories = ['retail_and_recreation_percent_change_from_baseline', 'grocery_and_pharmacy_percent_change_from_baseline', \\\n",
        "    'transit_stations_percent_change_from_baseline', 'workplaces_percent_change_from_baseline']\n",
        "\n",
        "#Iterate over all countries\n",
        "for country in df_google_mob_dem_index['country_region'].unique() :\n",
        "    #Keep only the values with sub_region_1 is NaN as it corresponds to data for the whole country\n",
        "    df_country = df_google_mob_dem_index[(df_google_mob_dem_index['country_region'] == country) & (df_google_mob_dem_index['sub_region_1'].isnull())]\n",
        "    #This list will contain the change between the lowest and the highest period for each category\n",
        "    changes_list = []\n",
        "\n",
        "    for category in categories :\n",
        "        #Extract the data corresponding to this category\n",
        "        category_data = df_country[category]\n",
        "        #Compute the highest and the lowest periods\n",
        "        highest_period_value = category_data.iloc[:30].rolling(nb_days_period).mean().max()\n",
        "        lowest_period_value = category_data.rolling(nb_days_period).mean().min()\n",
        "        #Compute the mobility change and add it to the list\n",
        "        category_change = lowest_period_value - highest_period_value\n",
        "        changes_list.append(category_change)\n",
        "\n",
        "    #Add the corresponding entry to the dictionnary\n",
        "    dict_restrictions[country] = changes_list\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert the dictinnary dict_restrictions into an array to use in order to use kmeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Import the corresponding modules\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#Convert the dictionnary into an array to use it with kmeans\n",
        "restrictions_array = np.empty((0, len(categories)))\n",
        "countries = []\n",
        "\n",
        "#Iterate over all countries\n",
        "for country in (dict_restrictions.keys()) :\n",
        "    country_restrictions = np.array(dict_restrictions[country])\n",
        "\n",
        "    #Check if the row of the corresponding country contains nan values\n",
        "    if np.isnan(country_restrictions).any() :\n",
        "        print ('Country containing nan values')\n",
        "        print (country)\n",
        "\n",
        "    #If no nan values add it to the array\n",
        "    else :\n",
        "        countries.append(country)\n",
        "        restrictions_array = A = np.vstack([restrictions_array, country_restrictions])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the elbow method to evaluate the best number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Define a function which plots the sse\n",
        "\n",
        "def plot_sse(features_X, start=2, end=11):\n",
        "    sse = []\n",
        "    for k in range(start, end):\n",
        "        # Assign the labels to the clusters\n",
        "        kmeans = KMeans(n_clusters=k, random_state=10).fit(features_X)\n",
        "        sse.append({\"k\": k, \"sse\": kmeans.inertia_})\n",
        "\n",
        "    sse = pd.DataFrame(sse)\n",
        "    # Plot the data\n",
        "    plt.plot(sse.k, sse.sse)\n",
        "    plt.xlabel(\"K\")\n",
        "    plt.ylabel(\"Sum of Squared Errors\")\n",
        "\n",
        "#Apply it to our data\n",
        "plot_sse(restrictions_array)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cluster the data with KMeans, with 5 clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#The best number of clusters is taken equal to 5\n",
        "n_clusters = 5\n",
        "\n",
        "#Performing k_means\n",
        "kmean = KMeans(n_clusters=n_clusters, random_state=2).fit(restrictions_array)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a plot summarizing the restrictions for the centroid of each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create a plot summurizing the centroid of each cluster\n",
        "\n",
        "#Create the figure\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "#Create the categories for the X axis\n",
        "X = ['Retail & recreation','Grocery & Pharmacy','Transit stations','Workplaces']\n",
        "X_axis = np.arange(len(X))\n",
        "\n",
        "#Use seaborn colors\n",
        "colors = sns.color_palette('deep')\n",
        "\n",
        "#Create the bar plots\n",
        "for i in range (kmean.cluster_centers_.shape[0]) :\n",
        "    plt.bar(X_axis + 0.12*i, kmean.cluster_centers_[i,:], 0.12, label = 'Cluster '+str(i+1), color=colors[i])\n",
        "\n",
        "#Titles, legends etc\n",
        "plt.xticks(X_axis+.15*1.5, X)\n",
        "plt.title(\"Decrease in the visits to different types of places\")\n",
        "plt.ylabel(\"Relative decrease (%)\")\n",
        "plt.legend(loc = 'lower right')\n",
        "#plt.savefig('images/categories_change.png')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Count the number of countries in each bin for democracy index. A bin is for instace [0.3, 0.4[ . The goal of this is to then comput the proportion of countries in each cluster of each bin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "#Create a list with the democracy indexes of all the countries\n",
        "all_dem_indexes = []\n",
        "\n",
        "#Create a list with all the democracy indexes\n",
        "for country in countries :\n",
        "    dem_index = df_google_mob_dem_index['dem_index'][df_google_mob_dem_index['country_region']==country].iloc[0]\n",
        "    all_dem_indexes.append(dem_index)\n",
        "\n",
        "#Count the number of countries in each bin using the previously created list\n",
        "countries_dem_index_bins = np.zeros(10)\n",
        "\n",
        "for i in range (len(all_dem_indexes)) :\n",
        "    dem_index = all_dem_indexes[i]\n",
        "    bin = math.floor(dem_index*10)\n",
        "    countries_dem_index_bins[bin] += 1\n",
        "\n",
        "\n",
        "#Plot the results\n",
        "plt.bar([.5+i for i in range(10)], countries_dem_index_bins)\n",
        "plt.title('Number of countries for each democracy index bin')\n",
        "plt.xlabel('Democracy index x10')\n",
        "plt.ylabel('Number of countries')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display detailed informations for each clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#For each cluster, find the corresponding countries and the mean democracy index\n",
        "\n",
        "#Getting sns colors for plots\n",
        "colors = sns.color_palette('deep')\n",
        "\n",
        "#Iterate over the 5 clusters\n",
        "for i in range (n_clusters) :\n",
        "\n",
        "    #Print all the countries which are in the current cluster\n",
        "    countries_cluster = [] \n",
        "    for j in range (kmean.labels_.shape[0]) :\n",
        "        if kmean.labels_[j] == i :\n",
        "            countries_cluster.append(countries[j])\n",
        "\n",
        "    print ('Cluster', i+1, ':')\n",
        "    print (countries_cluster)\n",
        "\n",
        "    #Compute the mean and median democracy index of the corresponding cluster\n",
        "    dem_indexes = []\n",
        "    for country in countries_cluster :\n",
        "        dem_index = df_google_mob_dem_index['dem_index'][df_google_mob_dem_index['country_region']==country].iloc[0]\n",
        "        dem_indexes.append(dem_index)\n",
        "\n",
        "    mean_dem_index = np.mean(np.array(dem_indexes))\n",
        "    median_dem_index = np.median(np.array(dem_indexes))\n",
        "\n",
        "    print ('Mean democracy index :', mean_dem_index)\n",
        "    print ('Median democracy index :', median_dem_index, '\\n')\n",
        "\n",
        "    #Print the restrictions for the centroid of the cluster\n",
        "    print ('Restrictions_characteristics')\n",
        "    for j in range (len(categories)) :\n",
        "        print (categories[j], kmean.cluster_centers_[i, j], '%')\n",
        "    \n",
        "    #For the current cluster, compute for each democracy index bin the proportion of countries contained in the cluster\n",
        "    cluster_dem_indexes_bins = np.zeros(10)\n",
        "    for j in range (len(dem_indexes)) :\n",
        "        dem_index = dem_indexes[j]\n",
        "        bin = math.floor(dem_index*10)\n",
        "        cluster_dem_indexes_bins[bin] += 1/ countries_dem_index_bins[bin]\n",
        "    \n",
        "    #Plot the corresponding ditribution\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar([.05+i*.1 for i in range(10)], cluster_dem_indexes_bins, 0.08, color = colors[i])\n",
        "    plt.xlabel('Democracy index')\n",
        "    plt.ylabel('Proportion of countries for each bin')\n",
        "    plt.title('Distribution of the democracy index for cluster ' + str(i+1))\n",
        "    filename = 'images/cluster_' + str(i+1) + '.png'\n",
        "    #plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "    print ('\\n')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "ca290eae0f5af857e3953b76ad34f9e6835e16bf594a023f95ab180c0e53d649"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
